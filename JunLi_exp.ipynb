{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dengjunli 的测试代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pca(X, n_components):\n",
    "    \"\"\"\n",
    "    对给定的数据集执行PCA并返回前n个主成分。\n",
    "    \n",
    "    :param X: 数据集，形状为(num_samples, num_features)\n",
    "    :param n_components: 保留的主成分数目\n",
    "    :return: 主成分矩阵，形状为(num_features, n_components)\n",
    "    \"\"\"\n",
    "    # 中心化数据\n",
    "    X_centered = X - X.mean(dim=0)\n",
    "    \n",
    "    # 计算协方差矩阵\n",
    "    covariance_matrix = torch.mm(X_centered.t(), X_centered) / (X_centered.size(0) - 1)\n",
    "    \n",
    "    # 求解特征值和特征向量\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(covariance_matrix, UPLO='U')\n",
    "    \n",
    "    # 选择主成分\n",
    "    idx = eigenvalues.argsort(descending=True)[:n_components]\n",
    "    principal_components = eigenvectors[:, idx]\n",
    "    \n",
    "    return principal_components\n",
    "\n",
    "def project_to_basis(X, basis):\n",
    "    \"\"\"\n",
    "    将数据集X投影到基向量basis上，并计算投影权重。\n",
    "    \n",
    "    :param X: 要投影的数据集，形状为(num_samples, num_features)\n",
    "    :param basis: 基向量，形状为(num_features, n_components)\n",
    "    :return: 投影权重，形状为(num_samples, n_components)\n",
    "    \"\"\"\n",
    "    X_centered = X - X.mean(dim=0)\n",
    "    weights = torch.mm(X_centered, basis)\n",
    "    return weights\n",
    "\n",
    "def reconstruct_from_weights(weights, basis):\n",
    "    \"\"\"\n",
    "    使用给定的权重和基向量重构数据。\n",
    "    \n",
    "    :param weights: 投影权重，形状为(num_samples, n_components)\n",
    "    :param basis: 基向量，形状为(num_features, n_components)\n",
    "    :return: 重构后的数据，形状为(num_samples, num_features)\n",
    "    \"\"\"\n",
    "    return torch.mm(weights, basis.t())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设A_flame和B_flame是两个FLAME参数矩阵，形状分别为(240, 120)和(num_samples, 120)\n",
    "# 选择要保留的主成分数目\n",
    "A_flame = torch.randn(240, 120)\n",
    "B_flame = torch.randn(1, 120)\n",
    "\n",
    "n_components = 28\n",
    "\n",
    "# 步骤1: 对A的FLAME参数执行PCA\n",
    "A_principal_components = pca(A_flame, n_components)\n",
    "\n",
    "# 步骤2: 计算B的FLAME参数在A的基向量上的投影权重\n",
    "B_weights = project_to_basis(B_flame, A_principal_components)\n",
    "\n",
    "# 步骤3: 使用权重重构B的FLAME参数\n",
    "B_reconstructed = reconstruct_from_weights(B_weights, A_principal_components)\n",
    "\n",
    "\n",
    "print(\"B_reconstructed.shape:\", B_reconstructed.shape)  # 输出: (100, 120) - 重构后的B的FLAME参数矩阵\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_basis_single(X, basis, mean):\n",
    "    \"\"\"\n",
    "    将单帧数据X投影到基向量basis上，并计算投影权重。\n",
    "    \n",
    "    :param X: 要投影的单帧数据，形状为(1, num_features)或(num_features,)\n",
    "    :param basis: 基向量，形状为(num_features, n_components)\n",
    "    :param mean: 数据集的均值，用于中心化，形状为(num_features,)\n",
    "    :return: 投影权重，形状为(1, n_components)\n",
    "    \"\"\"\n",
    "    # 确保X是二维的\n",
    "    if X.dim() == 1:\n",
    "        X = X.unsqueeze(0)\n",
    "    X_centered = X - mean\n",
    "    weights = torch.mm(X_centered, basis)\n",
    "    return weights\n",
    "\n",
    "def reconstruct_from_weights_single(weights, basis, mean):\n",
    "    \"\"\"\n",
    "    使用给定的权重和基向量重构单帧数据。\n",
    "    \n",
    "    :param weights: 投影权重，形状为(1, n_components)\n",
    "    :param basis: 基向量，形状为(num_features, n_components)\n",
    "    :param mean: 数据集的均值，用于重构后的数据中心化，形状为(num_features,)\n",
    "    :return: 重构后的单帧数据，形状为(1, num_features)\n",
    "    \"\"\"\n",
    "    reconstruction = torch.mm(weights, basis.t()) + mean\n",
    "    return reconstruction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特定文件夹中的所有PNG图像按时间戳顺序拼接成MP4视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "可视化点云图像，mesh\n",
    "拼接 output 文件夹中的图像，生成视频\n",
    "'''\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_images_to_video(base_folder):\n",
    "    \"\"\"\n",
    "    将每个子目录中的图像转换成视频文件。\n",
    "\n",
    "    参数:\n",
    "    - base_folder: 包含子目录的基础目录路径。\n",
    "    \"\"\"\n",
    "    # 遍历 base_folder 下的每个条目\n",
    "    for entry in os.listdir(base_folder):\n",
    "        subdir = os.path.join(base_folder, entry)\n",
    "        # 确保条目是一个目录\n",
    "        if os.path.isdir(subdir):\n",
    "            video_name = os.path.join(subdir, f'FLAME_driven_animation_{entry}.mp4')\n",
    "\n",
    "            images = [img for img in os.listdir(subdir) if img.endswith(\".png\")]\n",
    "            images = natsorted(images)  # 根据名称自然排序\n",
    "\n",
    "            if not images:\n",
    "                continue  # 如果没有图像，则跳过当前目录\n",
    "\n",
    "            frame = cv2.imread(os.path.join(subdir, images[0]))\n",
    "            height, width, layers = frame.shape\n",
    "\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # 定义编解码器\n",
    "            video = cv2.VideoWriter(video_name, fourcc, 25, (width, height))\n",
    "\n",
    "            for image in tqdm(images, desc=entry):\n",
    "                video.write(cv2.imread(os.path.join(subdir, image)))\n",
    "\n",
    "            video.release()  # 释放 VideoWriter 对象\n",
    "\n",
    "# 使用示例\n",
    "base_folder = './output'\n",
    "convert_images_to_video(base_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wav2vec 预测音素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC \n",
    "# from datasets import load_dataset\n",
    "import torch\n",
    "import soundfile as sf\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "\n",
    "# Read and process the input\n",
    "audio_input, sample_rate = sf.read(\"./test_data/英文tts.wav\")\n",
    "inputs = processor(audio_input, sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "# Decode id into string\n",
    "predicted_ids = torch.argmax(logits, axis=-1)      \n",
    "predicted_sentences = processor.batch_decode(predicted_ids)\n",
    "print(predicted_sentences)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入视频，选取与特定音素相匹配的帧\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import os\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 视频文件路径\n",
    "video_path = './test_data/初一-11.mp4'\n",
    "video_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "audio_output_path = f'./test_data/extracted_audio_{video_name}.wav'\n",
    "resampled_audio_output_path = f'./test_data/resampled_audio_{video_name}_16000Hz.wav'\n",
    "phoneme_of_interest = 'ʃ'\n",
    "\n",
    "# 加载视频并提取音频\n",
    "video_clip = VideoFileClip(video_path)\n",
    "audio_clip = video_clip.audio\n",
    "audio_clip.write_audiofile(audio_output_path, codec='pcm_s16le')\n",
    "\n",
    "# 使用librosa重新采样音频文件至16000Hz\n",
    "audio, sr = librosa.load(audio_output_path, sr=16000)\n",
    "sf.write(resampled_audio_output_path, audio, 16000)\n",
    "\n",
    "print(\"音频长度:\", librosa.get_duration(y=audio, sr=16000), \"秒\")\n",
    "\n",
    "# 使用Wav2Vec2进行音频处理和模型预测\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"./wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"./wav2vec2-xls-r-300m-timit-phoneme\")\n",
    "audio_input, sample_rate = sf.read(resampled_audio_output_path)\n",
    "inputs = processor(audio_input, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs.input_values, attention_mask=inputs.attention_mask).logits\n",
    "\n",
    "predicted_ids = torch.argmax(logits, axis=-1)\n",
    "predicted_phonemes = processor.batch_decode(predicted_ids)\n",
    "print(\"Predicted phonemes:\", predicted_phonemes)\n",
    "\n",
    "# 提取特定音素对应的帧并保存\n",
    "timestamps_for_phoneme = []\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "output_folder = f'./test_data/{video_name}_phoneme_{phoneme_of_interest}_frames'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "for i, phoneme in enumerate(predicted_phonemes[0]):\n",
    "    if phoneme_of_interest in phoneme:\n",
    "        timestamps_for_phoneme.append(i * 20)  # Assuming each timestep corresponds to 20ms\n",
    "\n",
    "\n",
    "print(f\"Extracting frames for phoneme '{phoneme_of_interest}' at timestamps: {timestamps_for_phoneme}\")\n",
    "\n",
    "for timestamp in tqdm(timestamps_for_phoneme, desc='Extracting frames'):\n",
    "    frame_id = int((timestamp / 1000) * frame_rate)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_id)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        cv2.imwrite(os.path.join(output_folder, f'frame_{frame_id}.jpg'), frame)\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_phonemes[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查可视化 FLAME 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_flame_file(file_path):\n",
    "    \"\"\"\n",
    "    加载 FLAME 文件并打印内容\n",
    "\n",
    "    参数:\n",
    "    - file_path: FLAME 文件的路径\n",
    "    \"\"\"\n",
    "    # 确保文件存在\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"文件 {file_path} 不存在。\")\n",
    "        return\n",
    "\n",
    "    # 加载文件\n",
    "    payload = torch.load(file_path)\n",
    "\n",
    "    # # 打印文件内容的摘要\n",
    "    # print(f\"已加载文件：{file_path}\")\n",
    "    # print(\"文件内容包括：\")\n",
    "    # for key in payload.keys():\n",
    "    #     print(f\" - {key}: 类型 {type(payload[key])}\")\n",
    "    #     # 这里可以根据需要进一步打印内容或内容的摘要\n",
    "\n",
    "    # 如果需要查看特定内容，可以直接访问\n",
    "    # 例如，如果文件中包含了 'flame' 键:\n",
    "    if 'flame' in payload:\n",
    "        print(\"\\nFLAME 参数详情：\")\n",
    "        flame_params = payload['flame']\n",
    "        for param, value in flame_params.items():\n",
    "            print(f\"  - {param}: {value}\")  # 可以根据实际情况调整以适当方式打印或处理这些值\n",
    "\n",
    "# 示例用法\n",
    "file_path = './metrical-tracker/output/justin/checkpoint/00341.frame'  # 替换为实际文件路径\n",
    "load_flame_file(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def load_and_print_opencv_params(file_path):\n",
    "    \"\"\"\n",
    "    加载 FLAME 文件并打印其中的 opencv 参数\n",
    "\n",
    "    参数:\n",
    "    - file_path: FLAME 文件的路径\n",
    "    \"\"\"\n",
    "    # 确保文件存在\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"文件 {file_path} 不存在。\")\n",
    "        return\n",
    "\n",
    "    # 加载文件\n",
    "    payload = torch.load(file_path)\n",
    "\n",
    "    # 检查并打印 opencv 参数\n",
    "    if 'opencv' in payload:\n",
    "        print(\"\\nOpenCV 参数详情：\")\n",
    "        opencv_params = payload['opencv']\n",
    "        if 'K' in opencv_params:\n",
    "            K = opencv_params['K'][0]  # 假设 K 存在并获取第一个（如果有多个）\n",
    "            print(f\"相机矩阵 K: \\n{K}\")\n",
    "        else:\n",
    "            print(\"相机矩阵 K 不存在于 opencv 参数中。\")\n",
    "    else:\n",
    "        print(\"opencv 参数不存在于文件中。\")\n",
    "\n",
    "    # 检查并打印图像尺寸 img_size\n",
    "    if 'img_size' in payload:\n",
    "        img_size = payload['img_size']\n",
    "        print(f\"\\n图像尺寸: {img_size}\")\n",
    "    else:\n",
    "        print(\"图像尺寸 img_size 不存在于文件中。\")\n",
    "\n",
    "# 示例用法\n",
    "file_path = './metrical-tracker/output/justin/checkpoint/00350.frame'  # 替换为实际文件路径\n",
    "load_and_print_opencv_params(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练过程合成为视频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "import os\n",
    "\n",
    "# 图片所在的文件夹\n",
    "image_folder = '/root/autodl-tmp/WassersteinGS/dataset/head_move/INITIAL_ROT_ALONG_FACE_2D_UNet2D_noNorm_noSigmoid_SinCos_5_chronological/train'\n",
    "# 视频的名称\n",
    "video_name = 'output_chronological.mp4'\n",
    "\n",
    "images = [img for img in os.listdir(image_folder) if img.endswith(\".jpg\")]\n",
    "# 如果图片是按照数字顺序命名的，可以使用以下代码来对图片进行排序\n",
    "images.sort(key=lambda x: int(x.split('.')[0]))\n",
    "\n",
    "# 创建一个ImageSequenceClip对象，fps参数表示每秒的帧数\n",
    "clip = ImageSequenceClip([os.path.join(image_folder, img) for img in images], fps=25)\n",
    "\n",
    "# 将视频保存为mp4格式\n",
    "clip.write_videofile(video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 删除多余文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 获取所有文件\n",
    "files = glob.glob('/root/autodl-tmp/WassersteinGS/dataset/**', recursive=True)\n",
    "\n",
    "# 定义文件名模式\n",
    "patterns = [r'chkpnt(\\d+)\\.pth$', r'point_cloud_3dgs(\\d+)\\.ply$']\n",
    "\n",
    "for file in tqdm(files):\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, file)\n",
    "        print(\"match:\", match)\n",
    "        if match and int(match.group(1)) < 145000:\n",
    "            os.remove(file)\n",
    "            print(\"删除\",file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_video_features: torch.Size([10, 128, 8, 6])\n",
      "self.positional_encoding: torch.Size([128, 8, 6])\n",
      "audio_features: torch.Size([10, 128, 2])\n",
      "query: torch.Size([10, 128, 256])\n",
      "key.shape: torch.Size([10, 128, 256])\n",
      "key.transpose(1, 2).shape: torch.Size([10, 256, 128])\n",
      "attended_features: torch.Size([10, 128, 256])\n",
      "attended_features: torch.Size([10, 128, 256])\n",
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FlattenAndLinear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FlattenAndLinear, self).__init__(in_channels=128)\n",
    "        # 假设输入数据的维度为 [batch_size, 10, 128, 256]\n",
    "        # 我们首先通过一个卷积层来提取特征，这里使用一个简单的一维卷积\n",
    "        self.conv1 = nn.Conv1d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        # 接着使用一个最大池化层来降低维度\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        # 然后通过一个全连接层将卷积层的输出映射到目标维度\n",
    "        self.fc = nn.Linear(64 * 128, 256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过卷积层和激活函数\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # 将卷积层输出的维度从 [batch_size, 64, 64] 调整为 [batch_size, 64 * 64]\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        # 通过全连接层和激活函数\n",
    "        x = F.relu(self.fc(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ImprovedCrossAttentionModel(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(ImprovedCrossAttentionModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_frames = 5  # Assuming each \"batch\" in 5*batchsize contains 5 frames\n",
    "        \n",
    "        # Positional Encoding for video features\n",
    "        self.positional_encoding_video = nn.Parameter(torch.randn((128, 8, 6), dtype=torch.float32))\n",
    "        self.positional_encoding_audio = nn.Parameter(torch.randn((128, 2), dtype=torch.float32))\n",
    "        \n",
    "        # Attention layers\n",
    "        self.query_proj = nn.Linear(2, 256)  # Project audio features to match the dimensionality\n",
    "        self.key_proj = nn.Linear(8 * 6, 256)    # Project video features after convolution and reshaping\n",
    "        self.value_proj = nn.Linear(8 * 6, 256)  # Same as key projection\n",
    "        \n",
    "        # Output layers to ensure output dimension is 5*batchsize, 256\n",
    "        self.final_linear = nn.Linear(256, 256)\n",
    "        # self.output_reshape = nn.Linear(self.num_frames * self.batch_size, 256)\n",
    "        self.output_reshape = FlattenAndLinear(128)\n",
    "\n",
    "    def forward(self, video_features, audio_features):\n",
    "        # video_features: (5*batch_size, 128, 8, 6)\n",
    "        # audio_features: (5*batch_size, 128, 2)\n",
    "        \n",
    "        # Applying convolutional layer to video features\n",
    "        video_features = video_features.view(-1, 128, 8, 6)  # Reshape to handle each frame independently\n",
    "        # video_features = self.conv_layer(batched_frames)\n",
    "\n",
    "        print(\"video_features:\",video_features.shape) # 10 128 8 6\n",
    "        print(\"self.positional_encoding:\",self.positional_encoding_video.shape) # 128 8 6\n",
    "        video_features += self.positional_encoding_video  # Adding positional encoding\n",
    "\n",
    "        \n",
    "        # Reshape and maintain spatial structure for attention\n",
    "        video_features = video_features.view(-1, 128, 8*6)  # Keep spatial dimensions together # 10 128 48\n",
    "        # video_features = video_features.permute(0, 2, 1).contiguous().view(-1, 10*8*6)  # Reshape for key/value projection\n",
    "        \n",
    "        audio_features = audio_features.view(-1, 128, 2)  # Just ensure dimensions are correct # 10 128 2\n",
    "        print(\"audio_features:\",audio_features.shape)\n",
    "        audio_features += self.positional_encoding_audio\n",
    "        \n",
    "        # Project features for attention\n",
    "        query = self.query_proj(audio_features) # 10 128 256\n",
    "        key = self.key_proj(video_features)\n",
    "        value = self.value_proj(video_features)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        print(\"query:\",query.shape) # query: torch.Size([10, 128, 256])\n",
    "        print(\"key.shape:\",key.shape) # key.shape: torch.Size([10, 256])\n",
    "        print(\"key.transpose(1, 2).shape:\",key.transpose(1, 2).shape)\n",
    "\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2)) / (256 ** 0.5)\n",
    "        attention = F.softmax(attention_scores, dim=-1)\n",
    "        attended_features = torch.bmm(attention, value)\n",
    "        \n",
    "        # Post-attention processing to match required output dimensions\n",
    "        attended_features = self.final_linear(attended_features)\n",
    "        print(\"attended_features:\",attended_features.shape)\n",
    "\n",
    "        output = self.output_reshape(attended_features)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#### Example usage (commented out)\n",
    "improved_model = ImprovedCrossAttentionModel(batch_size=2)\n",
    "video_features = torch.randn(5*2, 128, 8, 6)\n",
    "audio_features = torch.randn(5*2, 128, 2)\n",
    "output = improved_model(video_features, audio_features)\n",
    "print(output.shape)  # Expected shape: (5*batch_size, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_video_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mconv_video_features\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_video_features' is not defined"
     ]
    }
   ],
   "source": [
    "conv_video_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
